<!DOCTYPE html>
<html lang="">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | David  Beattie</title>
    <meta name="author" content="David  Beattie">
    <meta name="description" content="A portfolio website.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/publications/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">David </span>Beattie</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/davidbeattie_cv_0623.pdf">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content publications clearfix">
    <div class="publications">

<ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SAP</abbr>
    
  
  </div>

  <div id="10.1145/3385955.3407927" class="col-sm-8">
    
      <div class="title">Incorporating the Perception of Visual Roughness into the Design of Mid-Air Haptic Textures</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Frier, William,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Georgiou, Orestis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Long, Benjamin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ablart, Damien
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the ACM Symposium on Applied Perception, </em>
      
      
        2020.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Ultrasonic mid-air haptic feedback enables the tactile exploration of virtual objects in digital environments. However, an object’s shape and texture is perceived multimodally, commencing before tactile contact is made. Visual cues, such as the spatial distribution of surface elements, play a critical first step in forming an expectation of how a texture should feel. When rendering surface texture virtually, its verisimilitude is dependent on whether these visually inferred prior expectations are experienced during tactile exploration. To that end, our work proposes a method where the visual perception of roughness is integrated into the rendering algorithm of mid-air haptic texture feedback. We develop a machine learning model trained on crowd-sourced visual roughness ratings of texture images from the Penn Haptic Texture Toolkit (HaTT). We establish tactile roughness ratings for different mid-air haptic stimuli and match these ratings to our model’s output, creating an end-to-end automated visuo-haptic rendering algorithm. We validate our approach by conducting a user study to examine the utility of the mid-air haptic feedback. This work can be used to automatically create tactile virtual surfaces where the visual perception of texture roughness guides the design of mid-air haptic feedback.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="10.1145/3321335.3324944" class="col-sm-8">
    
      <div class="title">User Engagement for Mid-Air Haptic Interactions with Digital Signage</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Limerick, Hannah,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hayden, Richard,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Georgiou, Orestis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Müller, Jörg
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 8th ACM International Symposium on Pervasive Displays, </em>
      
      
        2019.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Interactive digital signage is increasingly deployed in urban environments, from airports and train stations, to cinemas and shopping malls, whilst their integration into public spaces introduces new possibilities in multimedia presentation. In this paper, we explore the impact of mid-air haptic feedback on user engagement during gesture-based interactions with digital posters. To that end, a user study with seventeen participants was undertaken with two independent variables: interactivity (high/low), and mid-air haptic cues (on/off), whilst user engagement and emotional affect was measured with respect to various metrics. In this first attempt to understand the significance of mid-air haptic interactions for digital signage, we found increased user engagement levels, comparable to, if not greater than those achieved by content gamification. In particular, our analysis suggests that mid-air haptic feedback significantly improved usability and aesthetic appeal in comparison to digital signage without haptic feedback. Similarly, a higher level of gamification was also found to boost user engagement and helped to offer more compelling experiences with digital signage.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IMWUT</abbr>
    
  
  </div>

  <div id="10.1145/3130901" class="col-sm-8">
    
      <div class="title">Exploring How Drivers Perceive Spatial Earcons in Automated Vehicles</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Halvey, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the ACM Journal on Interactive, Mobile, &amp; Wearable Ubiquitous Technologies, </em>
      
      
        2017.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Automated vehicles seek to relieve the human driver from primary driving tasks, but this substantially diminishes the connection between driver and vehicle compared to manual operation. At present, automated vehicles lack any form of continual, appropriate feedback to re-establish this connection and offer a feeling of control. We suggest that auditory feedback can be used to support the driver in this context. A preliminary field study that explored how drivers respond to existing auditory feedback in manual vehicles was first undertaken. We then designed a set of abstract, synthesised sounds presented spatially around the driver, known as Spatial Earcons, that represented different primary driving sounds e.g. acceleration. To evaluate their effectiveness, we undertook a driving simulator study in an outdoor setting using a real vehicle. Spatial Earcons performed as well as Existing Vehicle Sounds during automated and manual driving scenarios. Subjective responses suggested Spatial Earcons produced an engaging driving experience. This paper argues that entirely new synthesised primary driving sounds, such as Spatial Earcons, can be designed for automated vehicles to replace Existing Vehicle Sounds. This creates new possibilities for presenting primary driving information in automated vehicles using auditory feedback, in order to re-establish a connection between driver and vehicle.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">UbiComp</abbr>
    
  
  </div>

  <div id="10.1145/2750858.2807519" class="col-sm-8">
    
      <div class="title">A Comparison of Artificial Driving Sounds for Automated Vehicles</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Halvey, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, </em>
      
      
        2015.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As automated vehicles currently do not provide sufficient feedback relating to the primary driving task, drivers have no assurance that an automated vehicle has understood and can cope with upcoming traffic situations [16]. To address this we conducted two user evaluations to investigate auditory displays in automated vehicles using different types of sound cues related to the primary driving sounds: acceleration, deceleration/braking, gear changing and indicating. Our first study compared earcons, speech and auditory icons with existing vehicle sounds. Our findings suggested that earcons were an effective alternative to existing vehicle sounds for presenting information related to the primary driving task. Based on these findings a second study was conducted to further investigate earcons modulated by different sonic parameters to present primary driving sounds. We discovered that earcons containing naturally mapped sonic parameters such as pitch and timbre were as effective as existing sounds in a simulated automated vehicle.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CHI</abbr>
    
  
  </div>

  <div id="beattie2015adapting" class="col-sm-8">
    
      <div class="title">Adapting SatNav to Meet the Demands of Future Automated Vehicles</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Halvey, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and McCall, Roderick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM Conference on Human Factors in Computing Systems Workshop on Experiencing Autonomous Vehicles: Crossing the Boundaries Between a Drive and a Ride, </em>
      
      
        2015.
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="10.1145/2639189.2641206" class="col-sm-8">
    
      <div class="title">What’s Around The Corner? Enhancing Driver Awareness in Autonomous Vehicles via in-Vehicle Spatial Auditory Displays</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Halvey, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and McCall, Rod
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, </em>
      
      
        2014.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>There is currently a distinct lack of design consideration associated with autonomous vehicles and their impact on human factors. Research has yet to consider fully the impact felt by the driver when he/she is no longer in control of the vehicle [12]. We propose that spatialised auditory feedback could be used to enhance driver awareness to the intended actions of autonomous vehicles. We hypothesise that this feedback will provide drivers with an enhanced sense of control. This paper presents a driving simulator study where 5 separate auditory feedback methods are compared during both autonomous and manual driving scenarios. We found that our spatialised auditory presentation method alerted drivers to the intended actions of autonomous vehicles much more than all other methods and they felt significantly more in control during scenarios containing sound vs. no sound. Finally, that overall workload in autonomous vehicle scenarios was lower compared to manual vehicle scenarios.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="beattie_baillie_halvey_mccall_2013" class="col-sm-8">
    
      <div class="title">Maintaining a Sense of Control in Autonomous Vehicles via Auditory Feedback</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Halvey, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and McCall, Roderick
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In The Fourth Workshop on the Perceptual Quality of Systems, </em>
      
      
        2013.
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="10.1145/2037373.2037478" class="col-sm-8">
    
      <div class="title">Feeling the Next Track: Designing Mobile Music Player Previews</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Morton, Lee
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services, </em>
      
      
        2011.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a novel method of interaction for users to preview an audio track haptically. The preview enables users to "feel" the track they want to select, thus saving them from having to look at the screen or listen to the track before actually playing it. Our results show that users enjoyed the combination of audio and haptic feedback and that users would very much like to see this type of sensory collaboration being incorporated into their own mobile device.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="10.1145/2019335.2019336" class="col-sm-8">
    
      <div class="title">Feel What You Hear: Haptic Feedback as an Accompaniment to Mobile Music Playback</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Baillie, Lynne,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Beattie, David</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Morton, Lee
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of Interacting with Sound Workshop: Exploring Context-Aware, Local and Social Audio Applications, </em>
      
      
        2011.
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The paper describes the creation of a mobile music player accompanied with synchronised haptic feedback to create a novel method of audio playback on a mobile device. There has been extensive research into the development of audio haptic systems to enhance user interaction on mobile devices as well as providing useful information about users’ surroundings while way-finding. We investigated an alternative collaboration of haptics and audio feedback which we present here. We believe that this collaboration provides a novel and fun method of interaction with a music playback application.Our results show that users enjoyed the combination of audio and haptic feedback and that users would very much like to see this type of sensory collaboration being incorporated into their own mobile device.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

</div>

<div class="patents">

</div>

  </article>

  

  

</div>
      
    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 David  Beattie. This site uses the <a><href>al-folio</href></a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

  </body>
</html>
