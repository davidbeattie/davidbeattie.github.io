@inproceedings{10.1145/3385955.3407927,
	abbr = {SAP},
	author = {Beattie, David and Frier, William and Georgiou, Orestis and Long, Benjamin and Ablart, Damien},
	title = {Incorporating the Perception of Visual Roughness into the Design of Mid-Air Haptic Textures},
	year = {2020.},
	isbn = {9781450376181},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3385955.3407927},
	doi = {10.1145/3385955.3407927},
	abstract = {Ultrasonic mid-air haptic feedback enables the tactile exploration of virtual objects in digital environments. However, an object’s shape and texture is perceived multimodally, commencing before tactile contact is made. Visual cues, such as the spatial distribution of surface elements, play a critical first step in forming an expectation of how a texture should feel. When rendering surface texture virtually, its verisimilitude is dependent on whether these visually inferred prior expectations are experienced during tactile exploration. To that end, our work proposes a method where the visual perception of roughness is integrated into the rendering algorithm of mid-air haptic texture feedback. We develop a machine learning model trained on crowd-sourced visual roughness ratings of texture images from the Penn Haptic Texture Toolkit (HaTT). We establish tactile roughness ratings for different mid-air haptic stimuli and match these ratings to our model’s output, creating an end-to-end automated visuo-haptic rendering algorithm. We validate our approach by conducting a user study to examine the utility of the mid-air haptic feedback. This work can be used to automatically create tactile virtual surfaces where the visual perception of texture roughness guides the design of mid-air haptic feedback.},
	booktitle = {Proceedings of the ACM Symposium on Applied Perception, },
	articleno = {4},
	numpages = {10},
	keywords = {Haptics, Mid-air Haptic Devices, Visuo-Haptic Feedback, Image Processing, Texture Perception, Ultrasound, Machine Learning},
	location = {Virtual Event, USA},
	series = {SAP '20},
	selected={true}
}

@inproceedings{10.1145/3321335.3324944,
	author = {Limerick, Hannah and Hayden, Richard and Beattie, David and Georgiou, Orestis and M\"{u}ller, J\"{o}rg},
	title = {User Engagement for Mid-Air Haptic Interactions with Digital Signage},
	year = {2019.},
	isbn = {9781450367516},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3321335.3324944},
	doi = {10.1145/3321335.3324944},
	abstract = {Interactive digital signage is increasingly deployed in urban environments, from airports and train stations, to cinemas and shopping malls, whilst their integration into public spaces introduces new possibilities in multimedia presentation. In this paper, we explore the impact of mid-air haptic feedback on user engagement during gesture-based interactions with digital posters. To that end, a user study with seventeen participants was undertaken with two independent variables: interactivity (high/low), and mid-air haptic cues (on/off), whilst user engagement and emotional affect was measured with respect to various metrics. In this first attempt to understand the significance of mid-air haptic interactions for digital signage, we found increased user engagement levels, comparable to, if not greater than those achieved by content gamification. In particular, our analysis suggests that mid-air haptic feedback significantly improved usability and aesthetic appeal in comparison to digital signage without haptic feedback. Similarly, a higher level of gamification was also found to boost user engagement and helped to offer more compelling experiences with digital signage.},
	booktitle = {Proceedings of the 8th ACM International Symposium on Pervasive Displays, },
	articleno = {15},
	numpages = {7},
	keywords = {public displays, digital signage, ultrasound, pervasive displays, mid-air haptics, touchless interaction, haptics, e-commerce},
	location = {Palermo, Italy},
	series = {PerDis '19},
	selected={true}
}

@inproceedings{10.1145/3130901,
	abbr = {IMWUT},
	author = {Beattie, David and Baillie, Lynne and Halvey, Martin},
	title = {Exploring How Drivers Perceive Spatial Earcons in Automated Vehicles},
	year = {2017.}, 
	issue_date = {September 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {1},
	number = {3},
	url = {https://doi.org/10.1145/3130901},
	doi = {10.1145/3130901},
	abstract = {Automated vehicles seek to relieve the human driver from primary driving tasks, but this substantially diminishes the connection between driver and vehicle compared to manual operation. At present, automated vehicles lack any form of continual, appropriate feedback to re-establish this connection and offer a feeling of control. We suggest that auditory feedback can be used to support the driver in this context. A preliminary field study that explored how drivers respond to existing auditory feedback in manual vehicles was first undertaken. We then designed a set of abstract, synthesised sounds presented spatially around the driver, known as Spatial Earcons, that represented different primary driving sounds e.g. acceleration. To evaluate their effectiveness, we undertook a driving simulator study in an outdoor setting using a real vehicle. Spatial Earcons performed as well as Existing Vehicle Sounds during automated and manual driving scenarios. Subjective responses suggested Spatial Earcons produced an engaging driving experience. This paper argues that entirely new synthesised primary driving sounds, such as Spatial Earcons, can be designed for automated vehicles to replace Existing Vehicle Sounds. This creates new possibilities for presenting primary driving information in automated vehicles using auditory feedback, in order to re-establish a connection between driver and vehicle.},
	booktitle = {Proceedings of the ACM Journal on Interactive, Mobile, & Wearable Ubiquitous Technologies, },
	month = {September},
	articleno = {36},
	numpages = {24},
	keywords = {Auditory Feedback, Handover of Control, Auditory Displays, Existing Vehicle Sounds, Earcons, Driving Simulator, Automated Vehicles}
}

@inproceedings{10.1145/2750858.2807519,
	abbr = {UbiComp},
	author = {Beattie, David and Baillie, Lynne and Halvey, Martin},
	title = {A Comparison of Artificial Driving Sounds for Automated Vehicles},
	year = {2015.},
	isbn = {9781450335744},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2750858.2807519},
	doi = {10.1145/2750858.2807519},
	abstract = {As automated vehicles currently do not provide sufficient feedback relating to the primary driving task, drivers have no assurance that an automated vehicle has understood and can cope with upcoming traffic situations [16]. To address this we conducted two user evaluations to investigate auditory displays in automated vehicles using different types of sound cues related to the primary driving sounds: acceleration, deceleration/braking, gear changing and indicating. Our first study compared earcons, speech and auditory icons with existing vehicle sounds. Our findings suggested that earcons were an effective alternative to existing vehicle sounds for presenting information related to the primary driving task. Based on these findings a second study was conducted to further investigate earcons modulated by different sonic parameters to present primary driving sounds. We discovered that earcons containing naturally mapped sonic parameters such as pitch and timbre were as effective as existing sounds in a simulated automated vehicle.},
	booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, },
	pages = {451–462},
	numpages = {12},
	keywords = {auditory displays, driving simulator, earcons, automated vehicles, auditory icons, speech},
	location = {Osaka, Japan},
	series = {UbiComp '15}
}


@inproceedings{beattie2015adapting,

  title={Adapting SatNav to Meet the Demands of Future Automated Vehicles},
  abbr = {CHI},
  author={Beattie, David and Baillie, Lynne and Halvey, Martin and McCall, Roderick},
  booktitle={ACM Conference on Human Factors in Computing Systems Workshop on Experiencing Autonomous Vehicles: Crossing the Boundaries Between a Drive and a Ride, },
  year={2015.}
}

@inproceedings{10.1145/2639189.2641206,
	author = {Beattie, David and Baillie, Lynne and Halvey, Martin and McCall, Rod},
	title = {What's Around The Corner? Enhancing Driver Awareness in Autonomous Vehicles via in-Vehicle Spatial Auditory Displays},
	year = {2014.},
	isbn = {9781450325424},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2639189.2641206},
	doi = {10.1145/2639189.2641206},
	abstract = {There is currently a distinct lack of design consideration associated with autonomous vehicles and their impact on human factors. Research has yet to consider fully the impact felt by the driver when he/she is no longer in control of the vehicle [12]. We propose that spatialised auditory feedback could be used to enhance driver awareness to the intended actions of autonomous vehicles. We hypothesise that this feedback will provide drivers with an enhanced sense of control. This paper presents a driving simulator study where 5 separate auditory feedback methods are compared during both autonomous and manual driving scenarios. We found that our spatialised auditory presentation method alerted drivers to the intended actions of autonomous vehicles much more than all other methods and they felt significantly more in control during scenarios containing sound vs. no sound. Finally, that overall workload in autonomous vehicle scenarios was lower compared to manual vehicle scenarios.},
	booktitle = {Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational, },
	pages = {189–198},
	numpages = {10},
	keywords = {driving simulators, audio, autonomous vehicles, driver awareness, in-vehicle spatial auditory displays},
	location = {Helsinki, Finland},
	series = {NordiCHI '14}
}

@inproceedings{beattie_baillie_halvey_mccall_2013,
	author={Beattie, David and Baillie, Lynne and Halvey, Martin and McCall, Roderick},
	title={Maintaining a Sense of Control in Autonomous Vehicles via Auditory Feedback},
	year={2013.},
	location={Vienna, Austria}, 
	booktitle={The Fourth Workshop on the Perceptual Quality of Systems, }
}

@inproceedings{10.1145/2037373.2037478,
	author = {Beattie, David and Baillie, Lynne and Morton, Lee},
	title = {Feeling the Next Track: Designing Mobile Music Player Previews},
	year = {2011.},
	isbn = {9781450305419},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2037373.2037478},
	doi = {10.1145/2037373.2037478},
	abstract = {We present a novel method of interaction for users to preview an audio track haptically. The preview enables users to "feel" the track they want to select, thus saving them from having to look at the screen or listen to the track before actually playing it. Our results show that users enjoyed the combination of audio and haptic feedback and that users would very much like to see this type of sensory collaboration being incorporated into their own mobile device.},
	booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services, },
	pages = {659–662},
	numpages = {4},
	keywords = {audio playback, haptics, mobile interaction},
	location = {Stockholm, Sweden},
	series = {MobileHCI '11}
}

@inproceedings{10.1145/2019335.2019336,
	author = {Baillie, Lynne and Beattie, David and Morton, Lee},
	title = {Feel What You Hear: Haptic Feedback as an Accompaniment to Mobile Music Playback},
	year = {2011.},
	isbn = {9781450308830},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2019335.2019336},
	doi = {10.1145/2019335.2019336},
	abstract = {The paper describes the creation of a mobile music player accompanied with synchronised haptic feedback to create a novel method of audio playback on a mobile device. There has been extensive research into the development of audio haptic systems to enhance user interaction on mobile devices as well as providing useful information about users' surroundings while way-finding. We investigated an alternative collaboration of haptics and audio feedback which we present here. We believe that this collaboration provides a novel and fun method of interaction with a music playback application.Our results show that users enjoyed the combination of audio and haptic feedback and that users would very much like to see this type of sensory collaboration being incorporated into their own mobile device.},
	booktitle = {Proceedings of Interacting with Sound Workshop: Exploring Context-Aware, Local and Social Audio Applications, },
	pages = {1–6},
	numpages = {6},
	keywords = {haptics, mobile interaction, audio playback},
	location = {Stockholm, Sweden},
	series = {IwS '11}
}
